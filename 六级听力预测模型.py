# -*- coding: utf-8 -*-
"""
============================================================================
å…­çº§å¬åŠ›ç­”æ¡ˆé¢„æµ‹æ¨¡å‹
============================================================================
ä½œè€…: ä¼ åº·kk (Vx:1837620622)
é‚®ç®±: 2040168455@qq.com
å’¸é±¼/Bç«™: ä¸‡èƒ½ç¨‹åºå‘˜

é¢„æµ‹ç­–ç•¥ï¼š
1. åŠ æƒé¢‘ç‡æ¨¡å‹ - è¿‘å¹´æ•°æ®æƒé‡æ›´é«˜
2. é©¬å°”å¯å¤«é“¾ - åˆ†æé¢˜ç›®é—´è½¬ç§»æ¦‚ç‡
3. å¹³è¡¡çº¦æŸ - ç¡®ä¿ABCDåˆ†å¸ƒå‡è¡¡(æ¯ä¸ªçº¦6-7é¢˜)
4. åè¿ç»­æ¨¡å‹ - é¿å…è¿‡å¤šè¿ç»­ç›¸åŒç­”æ¡ˆ
5. é›†æˆæŠ•ç¥¨ - å¤šæ¨¡å‹ç»¼åˆå†³ç­–
============================================================================
"""

import pandas as pd
import numpy as np
from collections import Counter, defaultdict
import random
import warnings
warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ æ¨¡å‹
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import LabelEncoder
import xgboost as xgb

# ============================================================================
# æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# ============================================================================

def load_data(csv_path):
    """åŠ è½½CSVæ•°æ®"""
    df = pd.read_csv(csv_path, encoding='utf-8-sig')
    print(f"åŠ è½½æ•°æ®: {len(df)} å¥—è¯•å·")
    return df

def extract_answers(df):
    """æå–ç­”æ¡ˆçŸ©é˜µï¼ŒæŒ‰æ—¶é—´æ’åº"""
    # æŒ‰å¹´ä»½æœˆä»½å¥—æ•°æ’åº
    df = df.sort_values(['å¹´ä»½', 'æœˆä»½', 'å¥—æ•°']).reset_index(drop=True)
    
    # æå–25é¢˜ç­”æ¡ˆ
    answer_cols = [f'T{i}' for i in range(1, 26)]
    answers = df[answer_cols].values
    
    # æ—¶é—´ä¿¡æ¯
    times = df[['å¹´ä»½', 'æœˆä»½', 'å¥—æ•°']].values
    
    return answers, times, df

# ============================================================================
# æ¨¡å‹1: åŠ æƒé¢‘ç‡æ¨¡å‹ - è¿‘å¹´æ•°æ®æƒé‡æ›´é«˜
# ============================================================================

def weighted_frequency_model(answers, times, decay=0.85):
    """
    åŠ æƒé¢‘ç‡æ¨¡å‹
    è¶Šè¿‘çš„å¹´ä»½æƒé‡è¶Šé«˜ï¼Œä½¿ç”¨æŒ‡æ•°è¡°å‡
    """
    n_exams, n_questions = answers.shape
    predictions = []
    
    for q in range(n_questions):
        freq = {'A': 0, 'B': 0, 'C': 0, 'D': 0}
        
        for i in range(n_exams):
            # æƒé‡ï¼šè¶Šæ–°çš„æ•°æ®æƒé‡è¶Šé«˜
            weight = decay ** (n_exams - 1 - i)
            ans = answers[i, q]
            if ans in freq:
                freq[ans] += weight
        
        # é€‰æ‹©åŠ æƒé¢‘ç‡æœ€é«˜çš„é€‰é¡¹
        best = max(freq, key=freq.get)
        predictions.append(best)
    
    return predictions

# ============================================================================
# æ¨¡å‹2: é©¬å°”å¯å¤«é“¾æ¨¡å‹ - åˆ†æé¢˜ç›®é—´è½¬ç§»æ¦‚ç‡
# ============================================================================

def markov_model(answers):
    """
    é©¬å°”å¯å¤«é“¾æ¨¡å‹
    åŸºäºå‰ä¸€é¢˜ç­”æ¡ˆé¢„æµ‹å½“å‰é¢˜
    """
    n_exams, n_questions = answers.shape
    
    # æ„å»ºè½¬ç§»çŸ©é˜µ
    transitions = defaultdict(lambda: defaultdict(int))
    
    for exam in answers:
        for i in range(1, n_questions):
            prev_ans = exam[i-1]
            curr_ans = exam[i]
            transitions[prev_ans][curr_ans] += 1
    
    # è½¬æ¢ä¸ºæ¦‚ç‡
    trans_prob = {}
    for prev, nexts in transitions.items():
        total = sum(nexts.values())
        trans_prob[prev] = {k: v/total for k, v in nexts.items()}
    
    # ç¬¬ä¸€é¢˜ä½¿ç”¨é¢‘ç‡
    first_freq = Counter(answers[:, 0])
    first_ans = first_freq.most_common(1)[0][0]
    
    predictions = [first_ans]
    for i in range(1, n_questions):
        prev = predictions[-1]
        if prev in trans_prob:
            probs = trans_prob[prev]
            # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„
            next_ans = max(probs, key=probs.get)
        else:
            next_ans = 'A'
        predictions.append(next_ans)
    
    return predictions

# ============================================================================
# æ¨¡å‹3: ä½ç½®é¢‘ç‡æ¨¡å‹ - æ¯é¢˜ç‹¬ç«‹åˆ†æ
# ============================================================================

def position_frequency_model(answers):
    """
    ä½ç½®é¢‘ç‡æ¨¡å‹
    ç»Ÿè®¡æ¯ä¸ªä½ç½®ä¸Šå„é€‰é¡¹çš„å‡ºç°é¢‘ç‡
    """
    n_exams, n_questions = answers.shape
    predictions = []
    probabilities = []
    
    for q in range(n_questions):
        freq = Counter(answers[:, q])
        total = sum(freq.values())
        
        # è®¡ç®—æ¦‚ç‡
        probs = {opt: freq.get(opt, 0) / total for opt in 'ABCD'}
        probabilities.append(probs)
        
        # é€‰æ‹©é¢‘ç‡æœ€é«˜çš„
        best = freq.most_common(1)[0][0] if freq else 'A'
        predictions.append(best)
    
    return predictions, probabilities

# ============================================================================
# æ¨¡å‹4: å¹³è¡¡çº¦æŸæ¨¡å‹ - ç¡®ä¿ABCDåˆ†å¸ƒå‡è¡¡
# ============================================================================

def balanced_model(answers, probabilities):
    """
    å¹³è¡¡çº¦æŸæ¨¡å‹
    åœ¨ä¿è¯æ¯é¢˜å€¾å‘çš„åŒæ—¶ï¼Œç¡®ä¿æ•´ä½“ABCDåˆ†å¸ƒå‡è¡¡
    ç›®æ ‡ï¼šæ¯ä¸ªé€‰é¡¹çº¦6-7é¢˜ï¼ˆ25é¢˜/4é€‰é¡¹â‰ˆ6.25ï¼‰
    """
    n_questions = 25
    target_count = n_questions // 4  # æ¯ä¸ªé€‰é¡¹ç›®æ ‡æ•°é‡ï¼š6
    
    predictions = []
    counts = {'A': 0, 'B': 0, 'C': 0, 'D': 0}
    
    for q in range(n_questions):
        probs = probabilities[q].copy()
        
        # å¯¹å·²ç»è¾¾åˆ°ç›®æ ‡æ•°é‡çš„é€‰é¡¹è¿›è¡Œæƒ©ç½š
        remaining = n_questions - q
        for opt in 'ABCD':
            if counts[opt] >= target_count + 1:
                probs[opt] *= 0.3  # é™ä½æ¦‚ç‡
            elif counts[opt] >= target_count:
                probs[opt] *= 0.7
        
        # å¯¹è¿˜éœ€è¦æ›´å¤šçš„é€‰é¡¹è¿›è¡ŒåŠ æˆ
        for opt in 'ABCD':
            needed = target_count - counts[opt]
            if needed > 0 and remaining <= needed * 2:
                probs[opt] *= 1.5
        
        # é€‰æ‹©è°ƒæ•´åæ¦‚ç‡æœ€é«˜çš„
        best = max(probs, key=probs.get)
        predictions.append(best)
        counts[best] += 1
    
    return predictions

# ============================================================================
# æ¨¡å‹5: åè¿ç»­æ¨¡å‹ - é¿å…è¿‡å¤šè¿ç»­ç›¸åŒç­”æ¡ˆ
# ============================================================================

def anti_consecutive_model(base_predictions, probabilities, max_consecutive=3):
    """
    åè¿ç»­æ¨¡å‹
    é¿å…è¶…è¿‡max_consecutiveä¸ªè¿ç»­ç›¸åŒç­”æ¡ˆ
    """
    predictions = list(base_predictions)
    
    for i in range(len(predictions)):
        if i >= max_consecutive:
            # æ£€æŸ¥å‰é¢æ˜¯å¦æœ‰è¿ç»­ç›¸åŒç­”æ¡ˆ
            consecutive = 1
            for j in range(i-1, -1, -1):
                if predictions[j] == predictions[i]:
                    consecutive += 1
                else:
                    break
            
            if consecutive > max_consecutive:
                # é€‰æ‹©æ¬¡ä¼˜é€‰é¡¹
                probs = probabilities[i].copy()
                probs[predictions[i]] = 0
                if sum(probs.values()) > 0:
                    predictions[i] = max(probs, key=probs.get)
    
    return predictions

# ============================================================================
# æ¨¡å‹6: æ—¶é—´è¶‹åŠ¿æ¨¡å‹ - åˆ†æé€‰é¡¹éšæ—¶é—´çš„å˜åŒ–è¶‹åŠ¿
# ============================================================================

def trend_model(answers, times):
    """
    æ—¶é—´è¶‹åŠ¿æ¨¡å‹
    åˆ†ææ¯ä¸ªä½ç½®ä¸Šé€‰é¡¹çš„æ—¶é—´å˜åŒ–è¶‹åŠ¿
    """
    n_exams, n_questions = answers.shape
    predictions = []
    
    for q in range(n_questions):
        # è®¡ç®—æœ€è¿‘5å¥—è¯•å·çš„é¢‘ç‡å˜åŒ–
        recent = answers[-10:, q] if n_exams >= 10 else answers[:, q]
        recent_freq = Counter(recent)
        
        # ä½¿ç”¨æœ€è¿‘çš„è¶‹åŠ¿
        if recent_freq:
            best = recent_freq.most_common(1)[0][0]
        else:
            best = 'A'
        predictions.append(best)
    
    return predictions

# ============================================================================
# æœºå™¨å­¦ä¹ æ¨¡å‹ - ç‰¹å¾å·¥ç¨‹å’Œè®­ç»ƒ
# ============================================================================

def prepare_ml_features(answers, times):
    """
    ä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹å‡†å¤‡ç‰¹å¾
    ç‰¹å¾åŒ…æ‹¬ï¼šé¢˜ç›®ä½ç½®ã€å†å²é¢‘ç‡ã€å‰åé¢˜å…³ç³»ç­‰
    """
    n_exams, n_questions = answers.shape
    le = LabelEncoder()
    le.fit(['A', 'B', 'C', 'D'])
    
    X_all = []
    y_all = []
    
    for q in range(n_questions):
        for i in range(n_exams):
            features = []
            
            # ç‰¹å¾1: é¢˜ç›®ä½ç½® (one-hotç¼–ç ç®€åŒ–ä¸ºæ•°å€¼)
            features.append(q / 25.0)  # å½’ä¸€åŒ–ä½ç½®
            
            # ç‰¹å¾2: æ‰€å±section (1-8é¢˜, 9-15é¢˜, 16-25é¢˜)
            if q < 8:
                features.extend([1, 0, 0])
            elif q < 15:
                features.extend([0, 1, 0])
            else:
                features.extend([0, 0, 1])
            
            # ç‰¹å¾3: å†å²è¯¥ä½ç½®å„é€‰é¡¹é¢‘ç‡
            if i > 0:
                hist = answers[:i, q]
                for opt in ['A', 'B', 'C', 'D']:
                    features.append(np.mean(hist == opt))
            else:
                features.extend([0.25, 0.25, 0.25, 0.25])
            
            # ç‰¹å¾4: å‰ä¸€é¢˜ç­”æ¡ˆ (å¦‚æœæœ‰)
            if q > 0:
                prev_ans = answers[i, q-1]
                features.extend([1 if prev_ans == opt else 0 for opt in ['A', 'B', 'C', 'D']])
            else:
                features.extend([0, 0, 0, 0])
            
            # ç‰¹å¾5: å½“å‰è¯•å·å·²æœ‰ç­”æ¡ˆåˆ†å¸ƒ
            if q > 0:
                current_dist = Counter(answers[i, :q])
                for opt in ['A', 'B', 'C', 'D']:
                    features.append(current_dist.get(opt, 0) / q)
            else:
                features.extend([0, 0, 0, 0])
            
            # ç‰¹å¾6: å¹´ä»½å’Œæœˆä»½
            features.append((times[i, 0] - 2016) / 10.0)  # å½’ä¸€åŒ–å¹´ä»½
            features.append(times[i, 1] / 12.0)  # å½’ä¸€åŒ–æœˆä»½
            
            X_all.append(features)
            y_all.append(le.transform([answers[i, q]])[0])
    
    return np.array(X_all), np.array(y_all), le

def prepare_prediction_features(answers, times, le):
    """
    ä¸ºé¢„æµ‹å‡†å¤‡ç‰¹å¾ï¼ˆä½¿ç”¨æœ€åä¸€å¥—è¯•å·çš„æ¨¡å¼ï¼‰
    """
    n_exams, n_questions = answers.shape
    
    X_pred = []
    for q in range(n_questions):
        features = []
        
        # ç‰¹å¾1: é¢˜ç›®ä½ç½®
        features.append(q / 25.0)
        
        # ç‰¹å¾2: æ‰€å±section
        if q < 8:
            features.extend([1, 0, 0])
        elif q < 15:
            features.extend([0, 1, 0])
        else:
            features.extend([0, 0, 1])
        
        # ç‰¹å¾3: å†å²è¯¥ä½ç½®å„é€‰é¡¹é¢‘ç‡
        hist = answers[:, q]
        for opt in ['A', 'B', 'C', 'D']:
            features.append(np.mean(hist == opt))
        
        # ç‰¹å¾4: ä½¿ç”¨æœ€å¸¸è§çš„å‰ä¸€é¢˜ç­”æ¡ˆ
        if q > 0:
            prev_common = Counter(answers[:, q-1]).most_common(1)[0][0]
            features.extend([1 if prev_common == opt else 0 for opt in ['A', 'B', 'C', 'D']])
        else:
            features.extend([0, 0, 0, 0])
        
        # ç‰¹å¾5: å¹³å‡åˆ†å¸ƒ
        features.extend([0.25, 0.25, 0.25, 0.25])
        
        # ç‰¹å¾6: å¹´ä»½å’Œæœˆä»½ (é¢„æµ‹2025å¹´)
        features.append((2025 - 2016) / 10.0)
        features.append(6 / 12.0)
        
        X_pred.append(features)
    
    return np.array(X_pred)

# ============================================================================
# éšæœºæ£®æ—æ¨¡å‹
# ============================================================================

def random_forest_model(train_answers, train_times, test_q_features=None):
    """éšæœºæ£®æ—åˆ†ç±»å™¨"""
    X, y, le = prepare_ml_features(train_answers, train_times)
    
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        min_samples_split=5,
        random_state=42,
        n_jobs=-1
    )
    model.fit(X, y)
    
    # é¢„æµ‹
    if test_q_features is None:
        test_q_features = prepare_prediction_features(train_answers, train_times, le)
    
    predictions = model.predict(test_q_features)
    return [le.inverse_transform([p])[0] for p in predictions]

# ============================================================================
# XGBoostæ¨¡å‹
# ============================================================================

def xgboost_model(train_answers, train_times, test_q_features=None):
    """XGBoostæ¢¯åº¦æå‡æ ‘"""
    X, y, le = prepare_ml_features(train_answers, train_times)
    
    model = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        random_state=42,
        use_label_encoder=False,
        eval_metric='mlogloss'
    )
    model.fit(X, y)
    
    if test_q_features is None:
        test_q_features = prepare_prediction_features(train_answers, train_times, le)
    
    predictions = model.predict(test_q_features)
    return [le.inverse_transform([p])[0] for p in predictions]

# ============================================================================
# æ¢¯åº¦æå‡æ¨¡å‹
# ============================================================================

def gradient_boosting_model(train_answers, train_times, test_q_features=None):
    """æ¢¯åº¦æå‡åˆ†ç±»å™¨"""
    X, y, le = prepare_ml_features(train_answers, train_times)
    
    model = GradientBoostingClassifier(
        n_estimators=100,
        max_depth=5,
        learning_rate=0.1,
        random_state=42
    )
    model.fit(X, y)
    
    if test_q_features is None:
        test_q_features = prepare_prediction_features(train_answers, train_times, le)
    
    predictions = model.predict(test_q_features)
    return [le.inverse_transform([p])[0] for p in predictions]

# ============================================================================
# ç¥ç»ç½‘ç»œMLPæ¨¡å‹
# ============================================================================

def mlp_model(train_answers, train_times, test_q_features=None):
    """å¤šå±‚æ„ŸçŸ¥æœºç¥ç»ç½‘ç»œ"""
    X, y, le = prepare_ml_features(train_answers, train_times)
    
    model = MLPClassifier(
        hidden_layer_sizes=(128, 64, 32),
        activation='relu',
        solver='adam',
        max_iter=500,
        random_state=42,
        early_stopping=True,
        validation_fraction=0.1
    )
    model.fit(X, y)
    
    if test_q_features is None:
        test_q_features = prepare_prediction_features(train_answers, train_times, le)
    
    predictions = model.predict(test_q_features)
    return [le.inverse_transform([p])[0] for p in predictions]

# ============================================================================
# é€»è¾‘å›å½’æ¨¡å‹
# ============================================================================

def logistic_model(train_answers, train_times, test_q_features=None):
    """é€»è¾‘å›å½’åˆ†ç±»å™¨"""
    X, y, le = prepare_ml_features(train_answers, train_times)
    
    model = LogisticRegression(
        multi_class='multinomial',
        solver='lbfgs',
        max_iter=1000,
        random_state=42
    )
    model.fit(X, y)
    
    if test_q_features is None:
        test_q_features = prepare_prediction_features(train_answers, train_times, le)
    
    predictions = model.predict(test_q_features)
    return [le.inverse_transform([p])[0] for p in predictions]

# ============================================================================
# SVMæ”¯æŒå‘é‡æœºæ¨¡å‹
# ============================================================================

def svm_model(train_answers, train_times, test_q_features=None):
    """æ”¯æŒå‘é‡æœºåˆ†ç±»å™¨"""
    X, y, le = prepare_ml_features(train_answers, train_times)
    
    model = SVC(
        kernel='rbf',
        C=1.0,
        gamma='scale',
        random_state=42
    )
    model.fit(X, y)
    
    if test_q_features is None:
        test_q_features = prepare_prediction_features(train_answers, train_times, le)
    
    predictions = model.predict(test_q_features)
    return [le.inverse_transform([p])[0] for p in predictions]

# ============================================================================
# æœ´ç´ è´å¶æ–¯æ¨¡å‹
# ============================================================================

def naive_bayes_model(train_answers, train_times, test_q_features=None):
    """æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨"""
    X, y, le = prepare_ml_features(train_answers, train_times)
    
    # å°†ç‰¹å¾è½¬æ¢ä¸ºéè´Ÿå€¼ï¼ˆæœ´ç´ è´å¶æ–¯è¦æ±‚ï¼‰
    X_positive = X - X.min() + 0.001
    
    model = MultinomialNB(alpha=1.0)
    model.fit(X_positive, y)
    
    if test_q_features is None:
        test_q_features = prepare_prediction_features(train_answers, train_times, le)
    
    test_positive = test_q_features - X.min() + 0.001
    predictions = model.predict(test_positive)
    return [le.inverse_transform([p])[0] for p in predictions]

# ============================================================================
# N-gramåºåˆ—æ¨¡å‹
# ============================================================================

def ngram_model(answers, n=3):
    """
    N-gramæ¨¡å‹
    åŸºäºå‰n-1é¢˜é¢„æµ‹å½“å‰é¢˜
    """
    n_exams, n_questions = answers.shape
    predictions = []
    
    for q in range(n_questions):
        if q < n - 1:
            # å‰n-1é¢˜ä½¿ç”¨é¢‘ç‡æ¨¡å‹
            freq = Counter(answers[:, q])
            predictions.append(freq.most_common(1)[0][0])
        else:
            # ä½¿ç”¨n-gram
            ngram_freq = defaultdict(Counter)
            for exam in answers:
                context = tuple(exam[q-n+1:q])
                ngram_freq[context][exam[q]] += 1
            
            # è·å–æœ€å¯èƒ½çš„åç»­
            # ä½¿ç”¨æœ€è¿‘å‡ å¥—è¯•å·çš„ä¸Šä¸‹æ–‡
            recent_contexts = [tuple(answers[i, q-n+1:q]) for i in range(-min(5, n_exams), 0)]
            combined = Counter()
            for ctx in recent_contexts:
                if ctx in ngram_freq:
                    combined.update(ngram_freq[ctx])
            
            if combined:
                predictions.append(combined.most_common(1)[0][0])
            else:
                freq = Counter(answers[:, q])
                predictions.append(freq.most_common(1)[0][0])
    
    return predictions

# ============================================================================
# æ»‘åŠ¨çª—å£æ¨¡å‹ - åªç”¨æœ€è¿‘Nå¥—è¯•å·
# ============================================================================

def sliding_window_model(answers, times, window_size=8):
    """
    æ»‘åŠ¨çª—å£æ¨¡å‹
    åªä½¿ç”¨æœ€è¿‘window_sizeå¥—è¯•å·è¿›è¡Œé¢„æµ‹
    """
    n_exams, n_questions = answers.shape
    
    # ä½¿ç”¨æœ€è¿‘çš„è¯•å·
    recent = answers[-window_size:] if n_exams >= window_size else answers
    
    predictions = []
    for q in range(n_questions):
        freq = Counter(recent[:, q])
        predictions.append(freq.most_common(1)[0][0])
    
    return predictions

# ============================================================================
# æŒ‡æ•°åŠ æƒæ¨¡å‹ - æŒ‡æ•°è¡°å‡æƒé‡
# ============================================================================

def exponential_weighted_model(answers, times, alpha=0.9):
    """
    æŒ‡æ•°åŠ æƒæ¨¡å‹
    è¶Šè¿‘çš„è¯•å·æƒé‡è¶Šé«˜ï¼Œä½¿ç”¨æŒ‡æ•°è¡°å‡
    """
    n_exams, n_questions = answers.shape
    predictions = []
    
    for q in range(n_questions):
        weighted_freq = {'A': 0, 'B': 0, 'C': 0, 'D': 0}
        
        for i in range(n_exams):
            weight = alpha ** (n_exams - 1 - i)
            ans = answers[i, q]
            weighted_freq[ans] += weight
        
        predictions.append(max(weighted_freq, key=weighted_freq.get))
    
    return predictions

# ============================================================================
# è´å¶æ–¯æ¨æ–­æ¨¡å‹
# ============================================================================

def bayesian_model(answers, times):
    """
    è´å¶æ–¯æ¨æ–­æ¨¡å‹
    ä½¿ç”¨åéªŒæ¦‚ç‡è¿›è¡Œé¢„æµ‹
    """
    n_exams, n_questions = answers.shape
    predictions = []
    
    # å…ˆéªŒï¼šå‡åŒ€åˆ†å¸ƒ
    prior = {'A': 0.25, 'B': 0.25, 'C': 0.25, 'D': 0.25}
    
    for q in range(n_questions):
        # è®¡ç®—åéªŒæ¦‚ç‡
        freq = Counter(answers[:, q])
        total = sum(freq.values())
        
        posterior = {}
        for opt in 'ABCD':
            # ä¼¼ç„¶ * å…ˆéªŒ
            likelihood = (freq.get(opt, 0) + 1) / (total + 4)  # æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
            posterior[opt] = likelihood * prior[opt]
        
        # å½’ä¸€åŒ–
        total_post = sum(posterior.values())
        posterior = {k: v/total_post for k, v in posterior.items()}
        
        predictions.append(max(posterior, key=posterior.get))
    
    return predictions

# ============================================================================
# æŠ•ç¥¨é›†æˆæ¨¡å‹ - å¤šæ¨¡å‹æŠ•ç¥¨
# ============================================================================

def voting_ensemble_model(answers, times):
    """
    æŠ•ç¥¨é›†æˆæ¨¡å‹
    ç»¼åˆå¤šä¸ªåŸºç¡€æ¨¡å‹çš„é¢„æµ‹è¿›è¡ŒæŠ•ç¥¨
    """
    _, probabilities = position_frequency_model(answers)
    
    # è·å–å„æ¨¡å‹é¢„æµ‹
    models = {
        'weighted': weighted_frequency_model(answers, times),
        'position': position_frequency_model(answers)[0],
        'trend': trend_model(answers, times),
        'markov': markov_model(answers),
        'sliding': sliding_window_model(answers, times, 8),
        'exp_weighted': exponential_weighted_model(answers, times, 0.9),
        'bayesian': bayesian_model(answers, times),
    }
    
    predictions = []
    for q in range(25):
        votes = Counter()
        for name, pred in models.items():
            votes[pred[q]] += 1
        
        # é€‰æ‹©å¾—ç¥¨æœ€å¤šçš„
        predictions.append(votes.most_common(1)[0][0])
    
    return predictions

# ============================================================================
# åŠ æƒæŠ•ç¥¨é›†æˆ - æ ¹æ®å†å²è¡¨ç°åŠ æƒ
# ============================================================================

def weighted_voting_model(answers, times):
    """
    åŠ æƒæŠ•ç¥¨é›†æˆ
    æ ¹æ®å„æ¨¡å‹å†å²è¡¨ç°åˆ†é…æƒé‡
    """
    _, probabilities = position_frequency_model(answers)
    
    # æ¨¡å‹æƒé‡ï¼ˆåŸºäºå›æµ‹ç»“æœï¼‰
    weights = {
        'trend': 3.0,       # è¶‹åŠ¿æ¨¡å‹æƒé‡æœ€é«˜
        'weighted': 2.5,    # åŠ æƒé¢‘ç‡
        'markov': 2.0,      # é©¬å°”å¯å¤«
        'position': 1.5,    # ä½ç½®é¢‘ç‡
        'sliding': 2.0,     # æ»‘åŠ¨çª—å£
        'exp_weighted': 2.0,# æŒ‡æ•°åŠ æƒ
        'bayesian': 1.5,    # è´å¶æ–¯
    }
    
    # è·å–å„æ¨¡å‹é¢„æµ‹
    models = {
        'weighted': weighted_frequency_model(answers, times),
        'position': position_frequency_model(answers)[0],
        'trend': trend_model(answers, times),
        'markov': markov_model(answers),
        'sliding': sliding_window_model(answers, times, 8),
        'exp_weighted': exponential_weighted_model(answers, times, 0.9),
        'bayesian': bayesian_model(answers, times),
    }
    
    predictions = []
    for q in range(25):
        votes = {'A': 0, 'B': 0, 'C': 0, 'D': 0}
        for name, pred in models.items():
            votes[pred[q]] += weights.get(name, 1.0)
        
        # åŠ å…¥ä½ç½®é¢‘ç‡ä½œä¸ºå‚è€ƒ
        for opt in 'ABCD':
            votes[opt] += probabilities[q].get(opt, 0) * 1.5
        
        predictions.append(max(votes, key=votes.get))
    
    return predictions

# ============================================================================
# å‘¨æœŸæ¨¡å‹ - åˆ†æç­”æ¡ˆå‘¨æœŸæ€§
# ============================================================================

def periodic_model(answers, times):
    """
    å‘¨æœŸæ¨¡å‹
    åˆ†ææ˜¯å¦å­˜åœ¨å‘¨æœŸæ€§è§„å¾‹ï¼ˆå¦‚6æœˆ/12æœˆå·®å¼‚ï¼‰
    """
    n_exams, n_questions = answers.shape
    predictions = []
    
    # åˆ†ç¦»6æœˆå’Œ12æœˆè¯•å·
    june_mask = times[:, 1] == 6
    dec_mask = times[:, 1] == 12
    
    june_answers = answers[june_mask] if any(june_mask) else answers
    dec_answers = answers[dec_mask] if any(dec_mask) else answers
    
    # é¢„æµ‹ä¸‹ä¸€ä¸ª6æœˆï¼ˆ2025å¹´6æœˆï¼‰
    for q in range(n_questions):
        # ä½¿ç”¨6æœˆä»½çš„é¢‘ç‡
        freq = Counter(june_answers[:, q])
        if freq:
            predictions.append(freq.most_common(1)[0][0])
        else:
            freq = Counter(answers[:, q])
            predictions.append(freq.most_common(1)[0][0])
    
    return predictions

# ============================================================================
# åæ¨¡å¼æ¨¡å‹ - é¿å…é‡å¤æœ€è¿‘çš„ç­”æ¡ˆ
# ============================================================================

def anti_pattern_model(answers, times):
    """
    åæ¨¡å¼æ¨¡å‹
    å‡è®¾å‡ºé¢˜è€…ä¼šé¿å…ä¸æœ€è¿‘è¯•å·ç›¸åŒçš„ç­”æ¡ˆ
    """
    n_exams, n_questions = answers.shape
    predictions = []
    
    # æœ€è¿‘3å¥—è¯•å·çš„ç­”æ¡ˆ
    recent = answers[-3:] if n_exams >= 3 else answers
    
    for q in range(n_questions):
        # ç»Ÿè®¡æœ€è¿‘ç­”æ¡ˆ
        recent_ans = [recent[i, q] for i in range(len(recent))]
        recent_freq = Counter(recent_ans)
        
        # å…¨éƒ¨å†å²é¢‘ç‡
        all_freq = Counter(answers[:, q])
        
        # é™ä½æœ€è¿‘å‡ºç°è¿‡çš„é€‰é¡¹æƒé‡
        adjusted = {}
        for opt in 'ABCD':
            base = all_freq.get(opt, 0)
            penalty = recent_freq.get(opt, 0) * 0.3
            adjusted[opt] = max(base - penalty, 0.1)
        
        predictions.append(max(adjusted, key=adjusted.get))
    
    return predictions

# ============================================================================
# æ··åˆæ·±åº¦æ¨¡å‹ - ç»“åˆå¤šç§ç‰¹å¾
# ============================================================================

def hybrid_deep_model(train_answers, train_times):
    """
    æ··åˆæ·±åº¦æ¨¡å‹
    ä½¿ç”¨æ›´ä¸°å¯Œçš„ç‰¹å¾å’Œæ›´æ·±çš„ç½‘ç»œ
    """
    X, y, le = prepare_ml_features(train_answers, train_times)
    
    # æ·»åŠ æ›´å¤šç‰¹å¾
    n_exams, n_questions = train_answers.shape
    
    model = MLPClassifier(
        hidden_layer_sizes=(256, 128, 64, 32),
        activation='relu',
        solver='adam',
        max_iter=1000,
        random_state=42,
        early_stopping=True,
        validation_fraction=0.15,
        learning_rate='adaptive',
        alpha=0.001  # L2æ­£åˆ™åŒ–
    )
    model.fit(X, y)
    
    test_features = prepare_prediction_features(train_answers, train_times, le)
    predictions = model.predict(test_features)
    return [le.inverse_transform([p])[0] for p in predictions]

# ============================================================================
# é›†æˆæ¨¡å‹ - å¤šæ¨¡å‹æŠ•ç¥¨
# ============================================================================

def ensemble_predict(answers, times):
    """
    é›†æˆé¢„æµ‹
    ç»¼åˆå¤šä¸ªæ¨¡å‹çš„ç»“æœè¿›è¡ŒæŠ•ç¥¨
    """
    # è·å–å„æ¨¡å‹é¢„æµ‹
    pred_weighted = weighted_frequency_model(answers, times)
    pred_markov = markov_model(answers)
    pred_position, probabilities = position_frequency_model(answers)
    pred_balanced = balanced_model(answers, probabilities)
    pred_trend = trend_model(answers, times)
    
    # å¯¹å¹³è¡¡æ¨¡å‹åº”ç”¨åè¿ç»­çº¦æŸ
    pred_balanced = anti_consecutive_model(pred_balanced, probabilities)
    
    # æŠ•ç¥¨ï¼ˆç»™ä¸åŒæ¨¡å‹ä¸åŒæƒé‡ï¼‰
    weights = {
        'weighted': 2.0,    # åŠ æƒé¢‘ç‡æƒé‡é«˜
        'position': 1.5,    # ä½ç½®é¢‘ç‡
        'balanced': 2.0,    # å¹³è¡¡æ¨¡å‹æƒé‡é«˜
        'trend': 1.5,       # è¶‹åŠ¿æ¨¡å‹
        'markov': 1.0       # é©¬å°”å¯å¤«
    }
    
    final_predictions = []
    prediction_details = []
    
    for i in range(25):
        votes = {'A': 0, 'B': 0, 'C': 0, 'D': 0}
        
        votes[pred_weighted[i]] += weights['weighted']
        votes[pred_position[i]] += weights['position']
        votes[pred_balanced[i]] += weights['balanced']
        votes[pred_trend[i]] += weights['trend']
        votes[pred_markov[i]] += weights['markov']
        
        # åŠ å…¥æ¦‚ç‡ä¿¡æ¯ä½œä¸ºå‚è€ƒ
        for opt in 'ABCD':
            votes[opt] += probabilities[i].get(opt, 0) * 1.0
        
        best = max(votes, key=votes.get)
        final_predictions.append(best)
        
        # è®°å½•è¯¦æƒ…
        prediction_details.append({
            'question': i + 1,
            'prediction': best,
            'weighted': pred_weighted[i],
            'position': pred_position[i],
            'balanced': pred_balanced[i],
            'trend': pred_trend[i],
            'markov': pred_markov[i],
            'probs': probabilities[i],
            'confidence': votes[best] / sum(votes.values())
        })
    
    return final_predictions, prediction_details

# ============================================================================
# æ¦‚ç‡é‡‡æ ·é¢„æµ‹ - åŸºäºå†å²æ¦‚ç‡éšæœºé‡‡æ ·
# ============================================================================

def probabilistic_predict(probabilities, seed=None):
    """
    æ¦‚ç‡é‡‡æ ·é¢„æµ‹
    æ ¹æ®æ¯é¢˜çš„å†å²æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œéšæœºé‡‡æ ·
    åŒæ—¶ä¿è¯ABCDåˆ†å¸ƒç›¸å¯¹å‡è¡¡
    """
    if seed is not None:
        random.seed(seed)
        np.random.seed(seed)
    
    predictions = []
    counts = {'A': 0, 'B': 0, 'C': 0, 'D': 0}
    target = 6  # æ¯ä¸ªé€‰é¡¹ç›®æ ‡æ•°é‡
    
    for q in range(25):
        probs = probabilities[q].copy()
        
        # åŠ¨æ€è°ƒæ•´æ¦‚ç‡ä»¥ä¿æŒå¹³è¡¡
        remaining = 25 - q
        for opt in 'ABCD':
            if counts[opt] >= 7:  # å·²ç»å¤ªå¤š
                probs[opt] *= 0.3
            elif counts[opt] >= 6:
                probs[opt] *= 0.6
            
            # å¦‚æœæŸé€‰é¡¹è¿˜å·®å¾ˆå¤šï¼Œå¢åŠ å…¶æ¦‚ç‡
            needed = target - counts[opt]
            if needed > 0 and remaining <= needed * 2:
                probs[opt] *= 1.8
        
        # å½’ä¸€åŒ–
        total = sum(probs.values())
        probs = {k: v/total for k, v in probs.items()}
        
        # æŒ‰æ¦‚ç‡é‡‡æ ·
        opts = list(probs.keys())
        weights = [probs[o] for o in opts]
        choice = random.choices(opts, weights=weights, k=1)[0]
        
        predictions.append(choice)
        counts[choice] += 1
    
    return predictions

# ============================================================================
# ç”Ÿæˆä¸¤å¥—è¯•å·ï¼ˆæœ‰æ˜æ˜¾å·®å¼‚ï¼‰
# ============================================================================

def generate_two_sets(answers, times, probabilities):
    """
    ç”Ÿæˆä¸¤å¥—æœ‰å·®å¼‚çš„é¢„æµ‹è¯•å·
    ä½¿ç”¨ä¸åŒçš„éšæœºç§å­å’Œç­–ç•¥
    """
    # ç¬¬ä¸€å¥—ï¼šé›†æˆæ¨¡å‹ + æ¦‚ç‡é‡‡æ ·æ··åˆ
    pred1_ensemble, details = ensemble_predict(answers, times)
    pred1_prob = probabilistic_predict(probabilities, seed=2025)
    
    # æ··åˆä¸¤ç§é¢„æµ‹
    pred1 = []
    for i in range(25):
        probs = probabilities[i]
        if probs[pred1_ensemble[i]] > 0.35:
            pred1.append(pred1_ensemble[i])
        else:
            pred1.append(pred1_prob[i])
    
    # åº”ç”¨å¹³è¡¡çº¦æŸ
    pred1 = apply_balance_constraint(pred1, probabilities)
    
    # ç¬¬äºŒå¥—ï¼šä½¿ç”¨ä¸åŒç§å­çš„æ¦‚ç‡é‡‡æ ·
    pred2 = probabilistic_predict(probabilities, seed=2026)
    
    # ç¡®ä¿ä¸¤å¥—æœ‰è¶³å¤Ÿå·®å¼‚ï¼ˆè‡³å°‘5é¢˜ä¸åŒï¼‰
    diff_count = sum(1 for a, b in zip(pred1, pred2) if a != b)
    if diff_count < 5:
        # å¼ºåˆ¶å¢åŠ å·®å¼‚
        for i in range(25):
            if pred1[i] == pred2[i] and diff_count < 8:
                sorted_opts = sorted(probabilities[i].items(), key=lambda x: -x[1])
                if len(sorted_opts) >= 2:
                    pred2[i] = sorted_opts[1][0]
                    diff_count += 1
    
    return pred1, pred2, details

def generate_two_sets_with_seed(answers, times, probabilities, seed):
    """ä½¿ç”¨æŒ‡å®šç§å­ç”Ÿæˆä¸¤å¥—è¯•å·"""
    random.seed(seed)
    np.random.seed(seed)
    return generate_two_sets(answers, times, probabilities)

def apply_balance_constraint(predictions, probabilities):
    """åº”ç”¨å¹³è¡¡çº¦æŸï¼Œç¡®ä¿ABCDåˆ†å¸ƒåˆç†"""
    pred = list(predictions)
    counts = Counter(pred)
    
    # ç›®æ ‡ï¼šæ¯ä¸ªé€‰é¡¹5-7é¢˜
    for _ in range(10):  # æœ€å¤šè°ƒæ•´10æ¬¡
        need_adjust = False
        
        for opt in 'ABCD':
            if counts.get(opt, 0) > 8:  # å¤ªå¤š
                # æ‰¾ä¸€ä¸ªå¯ä»¥æ›¿æ¢çš„ä½ç½®
                for i in range(25):
                    if pred[i] == opt:
                        probs = probabilities[i]
                        sorted_opts = sorted(probs.items(), key=lambda x: -x[1])
                        for new_opt, _ in sorted_opts:
                            if new_opt != opt and counts.get(new_opt, 0) < 6:
                                pred[i] = new_opt
                                counts[opt] -= 1
                                counts[new_opt] = counts.get(new_opt, 0) + 1
                                need_adjust = True
                                break
                        if need_adjust:
                            break
            
            elif counts.get(opt, 0) < 4:  # å¤ªå°‘
                # æ‰¾ä¸€ä¸ªå¯ä»¥æ¢æˆè¿™ä¸ªé€‰é¡¹çš„ä½ç½®
                for i in range(25):
                    if pred[i] != opt and counts[pred[i]] > 6:
                        probs = probabilities[i]
                        if probs.get(opt, 0) > 0.15:
                            old = pred[i]
                            pred[i] = opt
                            counts[old] -= 1
                            counts[opt] = counts.get(opt, 0) + 1
                            need_adjust = True
                            break
        
        if not need_adjust:
            break
    
    return pred

# ============================================================================
# å¤šæ¨¡å‹å›æµ‹è¯„ä¼°
# ============================================================================

def backtest_all_models(answers, times):
    """
    å¤šæ¨¡å‹å›æµ‹è¯„ä¼°ï¼ˆåŒ…å«æœºå™¨å­¦ä¹ æ¨¡å‹ï¼‰
    é€å¹´æ»šåŠ¨å›æµ‹ï¼Œæ¯”è¾ƒå„æ¨¡å‹å‡†ç¡®ç‡
    """
    all_years = sorted(set(times[:, 0]))
    
    print(f"\n{'='*100}")
    print("  å¤šæ¨¡å‹é€å¹´å›æµ‹è¯„ä¼°ï¼ˆç»Ÿè®¡æ¨¡å‹ + æœºå™¨å­¦ä¹ æ¨¡å‹ï¼‰")
    print(f"{'='*100}")
    
    # ç»Ÿè®¡æ¨¡å‹
    stat_models = ['åŠ æƒé¢‘ç‡', 'ä½ç½®é¢‘ç‡', 'é©¬å°”å¯å¤«', 'è¶‹åŠ¿æ¨¡å‹', 'æ»‘åŠ¨çª—å£', 'æŒ‡æ•°åŠ æƒ', 'è´å¶æ–¯']
    # é›†æˆæ¨¡å‹
    ensemble_models = ['æŠ•ç¥¨é›†æˆ', 'åŠ æƒæŠ•ç¥¨']
    # å…¶ä»–æ¨¡å‹
    other_models = ['å‘¨æœŸæ¨¡å‹', 'åæ¨¡å¼', 'N-gram', 'éšæœºæ£®æ—', 'XGBoost', 'æ·±åº¦æ··åˆ']
    # æ‰€æœ‰æ¨¡å‹
    model_names = stat_models + ensemble_models + other_models
    
    # å­˜å‚¨å„æ¨¡å‹å„å¹´ç»“æœ
    model_results = {name: {'correct': 0, 'total': 0, 'yearly': {}} for name in model_names}
    yearly_comparison = []
    
    # ä»2018å¹´å¼€å§‹å›æµ‹ï¼ˆç¡®ä¿æœ‰è¶³å¤Ÿè®­ç»ƒæ•°æ®ï¼‰
    for test_year in all_years[2:]:
        train_mask = times[:, 0] < test_year
        test_mask = times[:, 0] == test_year
        
        if not any(train_mask) or not any(test_mask):
            continue
        
        train_answers = answers[train_mask]
        train_times = times[train_mask]
        test_answers = answers[test_mask]
        test_times = times[test_mask]
        
        print(f"\nå›æµ‹ {int(test_year)} å¹´ (è®­ç»ƒé›†: {len(train_answers)}å¥—)...", end=" ")
        
        # è·å–æ¦‚ç‡åˆ†å¸ƒ
        _, probabilities = position_frequency_model(train_answers)
        
        # ç»Ÿè®¡æ¨¡å‹é¢„æµ‹
        predictions = {
            'åŠ æƒé¢‘ç‡': weighted_frequency_model(train_answers, train_times),
            'ä½ç½®é¢‘ç‡': position_frequency_model(train_answers)[0],
            'é©¬å°”å¯å¤«': markov_model(train_answers),
            'è¶‹åŠ¿æ¨¡å‹': trend_model(train_answers, train_times),
            'æ»‘åŠ¨çª—å£': sliding_window_model(train_answers, train_times, 8),
            'æŒ‡æ•°åŠ æƒ': exponential_weighted_model(train_answers, train_times, 0.9),
            'è´å¶æ–¯': bayesian_model(train_answers, train_times),
        }
        
        # é›†æˆæ¨¡å‹é¢„æµ‹
        try:
            predictions['æŠ•ç¥¨é›†æˆ'] = voting_ensemble_model(train_answers, train_times)
        except Exception as e:
            predictions['æŠ•ç¥¨é›†æˆ'] = predictions['ä½ç½®é¢‘ç‡']
        
        try:
            predictions['åŠ æƒæŠ•ç¥¨'] = weighted_voting_model(train_answers, train_times)
        except Exception as e:
            predictions['åŠ æƒæŠ•ç¥¨'] = predictions['ä½ç½®é¢‘ç‡']
        
        # å…¶ä»–æ¨¡å‹
        try:
            predictions['å‘¨æœŸæ¨¡å‹'] = periodic_model(train_answers, train_times)
        except Exception as e:
            predictions['å‘¨æœŸæ¨¡å‹'] = predictions['ä½ç½®é¢‘ç‡']
        
        try:
            predictions['åæ¨¡å¼'] = anti_pattern_model(train_answers, train_times)
        except Exception as e:
            predictions['åæ¨¡å¼'] = predictions['ä½ç½®é¢‘ç‡']
        
        try:
            predictions['N-gram'] = ngram_model(train_answers, n=3)
        except Exception as e:
            predictions['N-gram'] = predictions['ä½ç½®é¢‘ç‡']
        
        try:
            predictions['éšæœºæ£®æ—'] = random_forest_model(train_answers, train_times)
        except Exception as e:
            predictions['éšæœºæ£®æ—'] = predictions['ä½ç½®é¢‘ç‡']
        
        try:
            predictions['XGBoost'] = xgboost_model(train_answers, train_times)
        except Exception as e:
            predictions['XGBoost'] = predictions['ä½ç½®é¢‘ç‡']
        
        try:
            predictions['æ·±åº¦æ··åˆ'] = hybrid_deep_model(train_answers, train_times)
        except Exception as e:
            predictions['æ·±åº¦æ··åˆ'] = predictions['ä½ç½®é¢‘ç‡']
        
        # è®¡ç®—å„æ¨¡å‹åœ¨è¯¥å¹´çš„å‡†ç¡®ç‡
        year_results = {'year': int(test_year), 'exams': len(test_answers)}
        
        for model_name, pred in predictions.items():
            correct = 0
            for exam in test_answers:
                correct += sum(1 for p, a in zip(pred, exam) if p == a)
            
            total = len(test_answers) * 25
            accuracy = correct / total * 100
            
            model_results[model_name]['correct'] += correct
            model_results[model_name]['total'] += total
            model_results[model_name]['yearly'][int(test_year)] = accuracy
            
            year_results[model_name] = accuracy
        
        yearly_comparison.append(year_results)
        print("å®Œæˆ")
    
    # æ‰“å°é€å¹´å¯¹æ¯”è¡¨æ ¼
    print(f"\n{'='*120}")
    print("é€å¹´å„æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”:")
    print("-" * 120)
    
    # æ˜¾ç¤ºè¡¨å¤´
    header = f"{'å¹´ä»½':^6}"
    for name in model_names:
        header += f" {name[:4]:^7}"
    print(header)
    print("-" * 120)
    
    for yr in yearly_comparison:
        row = f"{yr['year']:^6}"
        best_acc = max(yr[name] for name in model_names)
        for name in model_names:
            acc = yr[name]
            marker = "â˜…" if acc == best_acc else " "
            row += f" {acc:>4.0f}%{marker} "
        print(row)
    
    print("-" * 120)
    
    # è®¡ç®—æ€»ä½“å‡†ç¡®ç‡
    print(f"\n{'='*60}")
    print("æ€»ä½“å‡†ç¡®ç‡æ’å:")
    print("-" * 60)
    
    final_results = []
    for name in model_names:
        total_correct = model_results[name]['correct']
        total_questions = model_results[name]['total']
        avg_acc = total_correct / total_questions * 100 if total_questions > 0 else 0
        improvement = avg_acc - 25
        if name in stat_models:
            model_type = "ç»Ÿè®¡"
        elif name in ensemble_models:
            model_type = "é›†æˆ"
        else:
            model_type = "ML"
        final_results.append({
            'name': name,
            'type': model_type,
            'correct': total_correct,
            'total': total_questions,
            'accuracy': avg_acc,
            'improvement': improvement
        })
    
    # æŒ‰å‡†ç¡®ç‡æ’åº
    final_results.sort(key=lambda x: -x['accuracy'])
    
    for i, r in enumerate(final_results):
        medal = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"][i] if i < 3 else "  "
        sign = "+" if r['improvement'] >= 0 else ""
        type_tag = f"[{r['type']}]"
        print(f"{medal} {r['name']:^12} {type_tag:^6}: {r['accuracy']:>6.2f}% "
              f"({r['correct']}/{r['total']}é¢˜) [{sign}{r['improvement']:.2f}%]")
    
    print("-" * 60)
    print(f"ç†è®ºéšæœºæ¦‚ç‡: 25.00%")
    
    # è¿”å›æœ€ä½³æ¨¡å‹
    best_model = final_results[0]['name']
    best_type = final_results[0]['type']
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model} ({best_type}) - å‡†ç¡®ç‡ {final_results[0]['accuracy']:.2f}%")
    
    # å¯¼å‡ºå›æµ‹æŠ¥å‘Š
    export_backtest_report(final_results, yearly_comparison, model_names)
    
    return final_results, yearly_comparison

def export_backtest_report(final_results, yearly_comparison, model_names=None):
    """å¯¼å‡ºå›æµ‹æŠ¥å‘Šåˆ°CSV"""
    output_path = '/Users/chuankangkk/Downloads/å…­çº§å¬åŠ›/å…­çº§å¬åŠ›é¢„æµ‹åˆ†æ/å›æµ‹æŠ¥å‘Š.csv'
    
    # å‡†å¤‡æ•°æ®
    rows = []
    if model_names is None:
        model_names = ['åŠ æƒé¢‘ç‡', 'ä½ç½®é¢‘ç‡', 'é©¬å°”å¯å¤«', 'è¶‹åŠ¿æ¨¡å‹', 'éšæœºæ£®æ—', 'XGBoost', 'æ¢¯åº¦æå‡', 'MLPç¥ç»ç½‘ç»œ', 'é€»è¾‘å›å½’', 'N-gram']
    
    # é€å¹´æ•°æ®
    for yr in yearly_comparison:
        row = {'å¹´ä»½': yr['year'], 'è¯•å·æ•°': yr['exams']}
        for name in model_names:
            row[name] = f"{yr[name]:.1f}%"
        rows.append(row)
    
    # æ·»åŠ æ±‡æ€»è¡Œ
    summary_row = {'å¹´ä»½': 'æ€»è®¡', 'è¯•å·æ•°': sum(yr['exams'] for yr in yearly_comparison)}
    for r in final_results:
        summary_row[r['name']] = f"{r['accuracy']:.2f}%"
    rows.append(summary_row)
    
    df = pd.DataFrame(rows)
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"\nå›æµ‹æŠ¥å‘Šå·²å¯¼å‡º: {output_path}")

def backtest(answers, times, test_years=2):
    """ç®€åŒ–å›æµ‹ï¼ˆå…¼å®¹æ—§è°ƒç”¨ï¼‰"""
    return backtest_all_models(answers, times)

def detailed_comparison(answers, times, best_model_name='é›†æˆæ¨¡å‹'):
    """
    è¯¦ç»†å¯¹æ¯”ï¼šæ˜¾ç¤ºæ¯å¥—è¯•å·çš„é¢„æµ‹vsçœŸå®ç­”æ¡ˆ
    """
    all_years = sorted(set(times[:, 0]))
    
    print(f"\n{'='*70}")
    print(f"  è¯¦ç»†å¯¹æ¯”: {best_model_name} é¢„æµ‹ vs çœŸå®ç­”æ¡ˆ")
    print(f"{'='*70}")
    
    # é€å¹´å›æµ‹
    for test_year in all_years[2:]:
        train_mask = times[:, 0] < test_year
        test_mask = times[:, 0] == test_year
        
        if not any(train_mask) or not any(test_mask):
            continue
        
        train_answers = answers[train_mask]
        train_times = times[train_mask]
        test_answers = answers[test_mask]
        test_times = times[test_mask]
        
        # è·å–æ¦‚ç‡åˆ†å¸ƒ
        _, probabilities = position_frequency_model(train_answers)
        
        # ä½¿ç”¨é›†æˆæ¨¡å‹é¢„æµ‹
        if best_model_name == 'é›†æˆæ¨¡å‹':
            pred, _ = ensemble_predict(train_answers, train_times)
        elif best_model_name == 'åŠ æƒé¢‘ç‡':
            pred = weighted_frequency_model(train_answers, train_times)
        elif best_model_name == 'ä½ç½®é¢‘ç‡':
            pred, _ = position_frequency_model(train_answers)
        elif best_model_name == 'å¹³è¡¡çº¦æŸ':
            pred = balanced_model(train_answers, probabilities)
        else:
            pred, _ = ensemble_predict(train_answers, train_times)
        
        print(f"\nã€{int(test_year)}å¹´ã€‘")
        print("-" * 70)
        
        for i, (exam, time) in enumerate(zip(test_answers, test_times)):
            exam_name = f"{int(time[0])}å¹´{int(time[1])}æœˆç¬¬{int(time[2])}å¥—"
            real = ''.join(exam)
            predicted = ''.join(pred)
            
            # è®¡ç®—æ­£ç¡®é¢˜ç›®
            correct_positions = [j+1 for j in range(25) if pred[j] == exam[j]]
            wrong_positions = [j+1 for j in range(25) if pred[j] != exam[j]]
            correct_count = len(correct_positions)
            
            print(f"\n{exam_name}:")
            print(f"  é¢„æµ‹: {predicted}")
            print(f"  çœŸå®: {real}")
            
            # é€é¢˜å¯¹æ¯”
            comparison = ""
            for j in range(25):
                if pred[j] == exam[j]:
                    comparison += "âœ“"
                else:
                    comparison += "âœ—"
            print(f"  å¯¹æ¯”: {comparison}")
            print(f"  æ­£ç¡®: {correct_count}/25 ({correct_count/25*100:.1f}%)")
            if correct_count > 0:
                print(f"  å‘½ä¸­é¢˜å·: {correct_positions}")

# ============================================================================
# æ‰“å°é¢„æµ‹ç»“æœ
# ============================================================================

def print_predictions(pred1, pred2, year, month, details):
    """æ‰“å°é¢„æµ‹ç»“æœ"""
    print(f"\n{'='*60}")
    print(f"  {year}å¹´{month}æœˆ å…­çº§å¬åŠ›ç­”æ¡ˆé¢„æµ‹")
    print(f"{'='*60}")
    
    # ç¬¬ä¸€å¥—
    print(f"\nã€ç¬¬ä¸€å¥—è¯•å·é¢„æµ‹ã€‘")
    print(f"å®Œæ•´ç­”æ¡ˆ: {''.join(pred1)}")
    print(f"\nåˆ†é¢˜ç­”æ¡ˆ:")
    for i in range(5):
        start = i * 5
        end = start + 5
        row = '  '.join([f"T{j+1}:{pred1[j]}" for j in range(start, end)])
        print(f"  {row}")
    
    # ç»Ÿè®¡åˆ†å¸ƒ
    dist1 = Counter(pred1)
    print(f"\né€‰é¡¹åˆ†å¸ƒ: A:{dist1['A']} B:{dist1['B']} C:{dist1['C']} D:{dist1['D']}")
    
    # ç¬¬äºŒå¥—
    print(f"\n{'â”€'*60}")
    print(f"\nã€ç¬¬äºŒå¥—è¯•å·é¢„æµ‹ã€‘")
    print(f"å®Œæ•´ç­”æ¡ˆ: {''.join(pred2)}")
    print(f"\nåˆ†é¢˜ç­”æ¡ˆ:")
    for i in range(5):
        start = i * 5
        end = start + 5
        row = '  '.join([f"T{j+1}:{pred2[j]}" for j in range(start, end)])
        print(f"  {row}")
    
    # ç»Ÿè®¡åˆ†å¸ƒ
    dist2 = Counter(pred2)
    print(f"\né€‰é¡¹åˆ†å¸ƒ: A:{dist2['A']} B:{dist2['B']} C:{dist2['C']} D:{dist2['D']}")
    
    # ä¸¤å¥—å·®å¼‚
    diff_count = sum(1 for a, b in zip(pred1, pred2) if a != b)
    diff_positions = [i+1 for i, (a, b) in enumerate(zip(pred1, pred2)) if a != b]
    print(f"\nä¸¤å¥—å·®å¼‚: {diff_count}é¢˜ (ä½ç½®: {diff_positions})")
    
    # ç½®ä¿¡åº¦åˆ†æ
    print(f"\n{'â”€'*60}")
    print("ç½®ä¿¡åº¦åˆ†æï¼ˆæŒ‰é¢˜å·ï¼‰:")
    high_conf = [d for d in details if d['confidence'] >= 0.4]
    low_conf = [d for d in details if d['confidence'] < 0.3]
    
    if high_conf:
        print(f"  é«˜ç½®ä¿¡åº¦(>40%): T{', T'.join([str(d['question']) for d in high_conf])}")
    if low_conf:
        print(f"  ä½ç½®ä¿¡åº¦(<30%): T{', T'.join([str(d['question']) for d in low_conf])}")

# ============================================================================
# å¯¼å‡ºé¢„æµ‹ç»“æœ
# ============================================================================

def export_predictions(predictions_list, output_path):
    """å¯¼å‡ºé¢„æµ‹ç»“æœåˆ°CSV"""
    rows = []
    for pred in predictions_list:
        row = {
            'å¹´ä»½': pred['year'],
            'æœˆä»½': pred['month'],
            'å¥—æ•°': pred['set'],
            'è€ƒè¯•æ—¶é—´': f"{pred['year']}å¹´{pred['month']}æœˆ"
        }
        for i, ans in enumerate(pred['answers']):
            row[f'T{i+1}'] = ans
        row['å®Œæ•´ç­”æ¡ˆ'] = ''.join(pred['answers'])
        rows.append(row)
    
    df = pd.DataFrame(rows)
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"\né¢„æµ‹ç»“æœå·²å¯¼å‡º: {output_path}")

# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

def main():
    print("="*60)
    print("  å…­çº§å¬åŠ›ç­”æ¡ˆé¢„æµ‹ç³»ç»Ÿ")
    print("="*60)
    
    # åŠ è½½æ•°æ®
    csv_path = '/Users/chuankangkk/Downloads/å…­çº§å¬åŠ›/å…­çº§å¬åŠ›é¢„æµ‹åˆ†æ/å…­çº§å¬åŠ›ç­”æ¡ˆ_2025-12-04.csv'
    df = load_data(csv_path)
    
    # æå–ç­”æ¡ˆ
    answers, times, df = extract_answers(df)
    print(f"æ•°æ®èŒƒå›´: {int(times[0,0])}å¹´ - {int(times[-1,0])}å¹´")
    
    # è·å–æ¦‚ç‡åˆ†å¸ƒ
    _, probabilities = position_frequency_model(answers)
    
    # å¤šæ¨¡å‹å›æµ‹è¯„ä¼°
    final_results, yearly_comparison = backtest(answers, times, test_years=2)
    
    # è·å–æœ€ä½³æ¨¡å‹åç§°
    best_model_name = final_results[0]['name']
    
    # è¯¦ç»†å¯¹æ¯”
    detailed_comparison(answers, times, best_model_name)
    
    # ========================================================================
    # é¢„æµ‹2025å¹´6æœˆå’Œ12æœˆï¼ˆå„ä¸¤å¥—ï¼‰
    # ========================================================================
    
    all_predictions = []
    
    # ä½¿ç”¨æœ€ä½³æ¨¡å‹(è¶‹åŠ¿æ¨¡å‹)é¢„æµ‹ + æ¦‚ç‡é‡‡æ ·ç”Ÿæˆä¸¤å¥—
    def best_model_predict(probabilities, seed):
        """ä½¿ç”¨è¶‹åŠ¿æ¨¡å‹+æ¦‚ç‡é‡‡æ ·ç»„åˆé¢„æµ‹"""
        # è¶‹åŠ¿æ¨¡å‹é¢„æµ‹
        trend_pred = trend_model(answers, times)
        # æ¦‚ç‡é‡‡æ ·ä½œä¸ºç¬¬äºŒå¥—
        prob_pred = probabilistic_predict(probabilities, seed=seed)
        return trend_pred, prob_pred
    
    # 2025å¹´6æœˆ
    pred1_jun, pred2_jun = best_model_predict(probabilities, 20256)
    _, details_jun = ensemble_predict(answers, times)
    print_predictions(pred1_jun, pred2_jun, 2025, 6, details_jun)
    all_predictions.append({'year': 2025, 'month': 6, 'set': 1, 'answers': pred1_jun})
    all_predictions.append({'year': 2025, 'month': 6, 'set': 2, 'answers': pred2_jun})
    
    # 2025å¹´12æœˆ
    pred1_dec, pred2_dec = best_model_predict(probabilities, 202512)
    print_predictions(pred1_dec, pred2_dec, 2025, 12, details_jun)
    all_predictions.append({'year': 2025, 'month': 12, 'set': 1, 'answers': pred1_dec})
    all_predictions.append({'year': 2025, 'month': 12, 'set': 2, 'answers': pred2_dec})
    
    # 2026å¹´6æœˆ
    pred1_jun26, pred2_jun26 = best_model_predict(probabilities, 20266)
    print_predictions(pred1_jun26, pred2_jun26, 2026, 6, details_jun)
    all_predictions.append({'year': 2026, 'month': 6, 'set': 1, 'answers': pred1_jun26})
    all_predictions.append({'year': 2026, 'month': 6, 'set': 2, 'answers': pred2_jun26})
    
    # 2026å¹´12æœˆ
    pred1_dec26, pred2_dec26 = best_model_predict(probabilities, 202612)
    print_predictions(pred1_dec26, pred2_dec26, 2026, 12, details_jun)
    all_predictions.append({'year': 2026, 'month': 12, 'set': 1, 'answers': pred1_dec26})
    all_predictions.append({'year': 2026, 'month': 12, 'set': 2, 'answers': pred2_dec26})
    
    # å¯¼å‡ºé¢„æµ‹
    output_path = '/Users/chuankangkk/Downloads/å…­çº§å¬åŠ›/å…­çº§å¬åŠ›é¢„æµ‹åˆ†æ/å…­çº§å¬åŠ›é¢„æµ‹ç»“æœ_2025-2026.csv'
    export_predictions(all_predictions, output_path)
    
    # ========================================================================
    # ç»Ÿè®¡åˆ†æ
    # ========================================================================
    
    print(f"\n{'='*60}")
    print("å†å²æ•°æ®ç»Ÿè®¡åˆ†æ")
    print(f"{'='*60}")
    
    print("\nå„é¢˜ä½ç½®é€‰é¡¹é¢‘ç‡åˆ†å¸ƒ:")
    print("-" * 60)
    print(f"{'é¢˜å·':^6} {'A':^10} {'B':^10} {'C':^10} {'D':^10} {'æœ€å¸¸è§':^8}")
    print("-" * 60)
    
    for q in range(25):
        probs = probabilities[q]
        most_common = max(probs, key=probs.get)
        print(f"T{q+1:>2}    {probs['A']*100:>6.1f}%   {probs['B']*100:>6.1f}%   "
              f"{probs['C']*100:>6.1f}%   {probs['D']*100:>6.1f}%    {most_common}")
    
    print(f"\n{'='*60}")
    print("é¢„æµ‹å®Œæˆï¼ç¥è€ƒè¯•é¡ºåˆ©ï¼")
    print("="*60)

if __name__ == '__main__':
    main()
